{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGE\n",
    "import os, sys\n",
    "\n",
    "# DATA\n",
    "# directory of the data\n",
    "datafile='./data/NL_Veluwe_2017_S1_bsc' # Sentinel-1 backscattering\n",
    "#datafile='./data/NL_Veluwe_2017_S1_coh' # Sentinel-1 coherence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameters\n",
    "rootname = datafile+'_cropselect'\n",
    "nclass = 5\n",
    "nepoch = 80\n",
    "nrun = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 / Agreggate the crop by categories \n",
    " Remove unuseful columns, aggragate the classes, set a numerical label and save results with '_cropselect.csv' at the end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd \n",
    "\n",
    "#load the data\n",
    "df = pd.read_csv(datafile+ '.csv')\n",
    "\n",
    "#remo small parcels\n",
    "df = df[df.area > 1000]\n",
    "\n",
    "# remove empty lines\n",
    "df= df[df.iloc[:,3] > 0]\n",
    "\n",
    "# remove unused columns\n",
    "df.drop(['.geo', 'area', 'gws_gewas','bufferedarea', 'id', 'perimeter'], axis=1, inplace=True)\n",
    "\n",
    "df[\"gws_gewasc\"] = df.gws_gewasc.astype(int)\n",
    "\n",
    "# aggregate the crops by classes\n",
    "gra = df['gws_gewasc'].isin([265, 266, 331, 336, 383, 332])\n",
    "mai = df['gws_gewasc'].isin([259, 316, 317])\n",
    "cer = df['gws_gewasc'].isin([234, 236, 235, 237])\n",
    "pot = df['gws_gewasc'].isin([2016, 2015 , 2017, 2014 ]) #2014, 2015, 2016, \n",
    "\n",
    "# convert the class to label integer strating from 0,1,2...\n",
    "df['gws_gewasc'] = 4\n",
    "df.loc[gra, 'gws_gewasc'] = 0\n",
    "df.loc[mai, 'gws_gewasc'] = 1\n",
    "df.loc[cer, 'gws_gewasc'] = 2\n",
    "df.loc[pot, 'gws_gewasc'] = 3\n",
    "\n",
    "# remove unlabelled parcels\n",
    "subset = df.loc[df['gws_gewasc'].isin([0,1,2,3,4])]\n",
    "\n",
    "# convert 'gws_gewasc' to 'label' and remove 'gws_gewasc'\n",
    "subset.insert(1, 'label', subset['gws_gewasc'])\n",
    "subset.drop(['gws_gewasc'],axis=1, inplace=True)\n",
    "\n",
    "# save the outptu as a .csv file\n",
    "subset.to_csv(datafile + '_cropselect.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.   / Select the training and test data\n",
    "Select the training data by sampling in the different classes and save as outpus 1 file for training and 1 file for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "samplesizeGRA=300\n",
    "samplesizeMAI=300\n",
    "samplesizeCER=150\n",
    "\n",
    "df = pd.read_csv(datafile + '_cropselect.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_nrun in range(nrun):\n",
    "    trainingGRA=df.loc[df['label'] == 0].take(np.random.permutation(len(df.loc[df['label'] == 0]))[:samplesizeGRA])\n",
    "    trainingMAI=df.loc[df['label'] == 1].take(np.random.permutation(len(df.loc[df['label'] == 1]))[:samplesizeMAI])\n",
    "    trainingCER=df.loc[df['label'] == 2].take(np.random.permutation(len(df.loc[df['label'] == 2]))[:samplesizeCER])\n",
    "    training=trainingGRA.append(trainingMAI).append(trainingCER)\n",
    "    testing = df.drop(training.index)\n",
    "    # save outptus\n",
    "    training.to_csv(datafile+'_cropselect_train_{}'.format(len(training))+'_nrun{}'.format(i_nrun)+'.csv')\n",
    "    testing.to_csv(datafile+'_cropselect_test_{}'.format(len(testing))+'_nrun{}'.format(i_nrun)+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 / Learning : building the neural network model, training the model, applying it to the parcel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "# Load CSV file, indicate that the first column represents labels\n",
    "from tflearn.data_utils import load_csv\n",
    "\n",
    "# tflearn.init_graph(gpu_memory_fraction=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1519  | total loss: \u001b[1m\u001b[32m0.37942\u001b[0m\u001b[0m | time: 0.043s\n",
      "| Adam | epoch: 080 | loss: 0.37942 - acc: 0.8680 -- iter: 576/600\n",
      "Training Step: 1520  | total loss: \u001b[1m\u001b[32m0.37938\u001b[0m\u001b[0m | time: 0.045s\n",
      "| Adam | epoch: 080 | loss: 0.37938 - acc: 0.8729 -- iter: 600/600\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "for i_nrun in range(nrun):\n",
    "    tf.reset_default_graph() #reset before starting\n",
    "    # load training\n",
    "    flist = glob.glob(rootname + '_train_*_nrun{}.csv'.format(i_nrun))\n",
    "    if len(flist) > 1:\n",
    "      print(\"FATAL: Only single training set allowed for {}, found {}\"\n",
    "            .format(rootname, len(flist)))\n",
    "      sys.exit(1)\n",
    "    elif len(flist) == 0:\n",
    "      print(\"FATAL: No training set found for {}\".format(rootname))\n",
    "      sys.exit(1)\n",
    "\n",
    "    # load testing\n",
    "    glist = glob.glob(rootname + '_test_*_nrun{}.csv'.format(i_nrun))\n",
    "    if len(glist) > 1:\n",
    "      print(\"FATAL: Only single test set allowed for {}, found {}\"\n",
    "            .format(rootname, len(flist)))\n",
    "      sys.exit(1)\n",
    "    elif len(glist) == 0:\n",
    "      print(\"FATAL: No test set found for {}\".format(rootname))\n",
    "      sys.exit(1)\n",
    "\n",
    "    fname = flist[0]\n",
    "    gname = glist[0]\n",
    "\n",
    "    data, labels = load_csv(fname, target_column=3,\n",
    "                            categorical_labels=True, n_classes=nclass)\n",
    "\n",
    "    test_data, test_labels = load_csv(gname, target_column=3,\n",
    "                            categorical_labels=True, n_classes=nclass)\n",
    "\n",
    "    # Preprocessing function\n",
    "    def preprocess(profiles, columns_to_delete):\n",
    "        # Sort by descending id and delete columns\n",
    "        for column_to_delete in sorted(columns_to_delete, reverse=True):\n",
    "            [profile.pop(column_to_delete) for profile in profiles]\n",
    "        return np.array(profiles, dtype=np.float32)\n",
    "\n",
    "    # Ignore 'id' \n",
    "    to_ignore=[0,1,2]\n",
    "\n",
    "    # Preprocess data\n",
    "    data = preprocess(data, to_ignore)\n",
    "\n",
    "    # Build neural network\n",
    "    net = tflearn.input_data(shape=[None, len(data[0])])\n",
    "    net = tflearn.fully_connected(net, 32)\n",
    "    net = tflearn.fully_connected(net, 32)\n",
    "    net = tflearn.fully_connected(net, nclass, activation='softmax')\n",
    "    net = tflearn.regression(net)\n",
    "\n",
    "    # Define model\n",
    "    model = tflearn.DNN(net)\n",
    "    # Start training (apply gradient descent algorithm)\n",
    "    model.fit(data, labels, n_epoch=nepoch, batch_size=32, show_metric=True)\n",
    "\n",
    "\n",
    "    fw = open('{}_{}_predictions.csv'.format(rootname, i_nrun), 'w')\n",
    "    fw.write(\"id,klass\")\n",
    "    for i in range(nclass):\n",
    "      fw.write(\",prob{}\".format(i))\n",
    "\n",
    "    fw.write('\\n')\n",
    "\n",
    "    # Check predictions for the samples not used in training\n",
    "    for i in range(len(test_data)):\n",
    "      sample = test_data[i][3:]\n",
    "      slabel = test_labels[i].tolist().index(1)\n",
    "      #print(labels[i])\n",
    "      pred = model.predict([sample])\n",
    "      fw.write(\"{},{}\".format(test_data[i][2], str(slabel)))\n",
    "      for i in range(nclass):\n",
    "        fw.write(\",{:6.2f}\".format(100*pred[0][i]))\n",
    "      fw.write('\\n')\n",
    "\n",
    "\n",
    "    tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4/ Select the class with the highest probability for eqch parcel and each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "for i_nrun in range(nrun):\n",
    "    # load the predictions probability\n",
    "    df = pd.read_csv(glob.glob(rootname +\n",
    "        '_{}_predictions.csv'\n",
    "        .format(i_nrun))[0], index_col=0, low_memory = False)\n",
    "    # index\n",
    "    r_index = df.columns[1:]\n",
    "    # Select the class with the proability maximum as the prediction\n",
    "    df['pred'] = df.apply(lambda x: np.array(x[r_index]).argmax(), axis=1)\n",
    "    # Get the maximum probability\n",
    "    df['pred_max'] = df.apply(lambda x: np.array(x[r_index]).max(), axis=1)\n",
    "    \n",
    "    # When the maximum probability is below 70 %, do not change the class prediction \n",
    "    df_ok = df[df.pred_max > 70]\n",
    "    df_nok = df[df.pred_max <= 70]\n",
    "    df_nok['pred']=df_nok['klass']\n",
    "    df=df_ok.append(df_nok)\n",
    "    \n",
    "    df.drop(r_index, axis=1, inplace=True)\n",
    "    df.drop('pred_max', axis=1, inplace=True)\n",
    "    \n",
    "    # save output\n",
    "    df.to_csv(rootname+ '_{}_class.csv'.format(i_nrun))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 / Combine the different run and retrieve the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv('{}_0_class.csv'.format(rootname), index_col = 0, low_memory=False)\n",
    "df1 = pd.read_csv('{}_1_class.csv'.format(rootname), index_col = 0, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the join and retain 'klass' label as 'klass_1'\n",
    "df = df0.join(df1, how=\"outer\", rsuffix= '_1')\n",
    "# Records that were not yet in df0 have 'klass' label missing (NA)\n",
    "# so, overwrite with those of 'klass_1'\n",
    "df['klass'].loc[df['klass'].isnull()] = df['klass_1'].loc[df['klass'].isnull()]\n",
    "# and drop the now redundant 'klass_1' label\n",
    "df.drop('klass_1', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(-1, inplace=True)\n",
    "r_index = df.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['majclass'] = df.apply(lambda x: Counter(x[r_index]).most_common(1)[0][0], axis=1)\n",
    "df['majcount'] = df.apply(lambda x: Counter(x[r_index]).most_common(1)[0][1], axis=1)\n",
    "\n",
    "df.astype(int).to_csv('{}_classes.csv'.format(rootname))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
